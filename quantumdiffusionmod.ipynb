{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Quantum-Classical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio del modello quantum diffusion su un sottoinsieme di MNIST...\n",
      "Esecuzione del modello latente su: cpu\n",
      "Caricamento del dataset MNIST...\n",
      "Utilizzando il 10.0% del dataset: 7000 esempi\n",
      "Addestramento dell'autoencoder per ridurre la dimensionalità a 8...\n",
      "Inizio addestramento dell'autoencoder per 20 epoche...\n",
      "Epoca 1/20, Loss: 0.094651\n",
      "Epoca 2/20, Loss: 0.062414\n",
      "Epoca 3/20, Loss: 0.056282\n",
      "Epoca 4/20, Loss: 0.053329\n",
      "Epoca 5/20, Loss: 0.048742\n",
      "Epoca 6/20, Loss: 0.043831\n",
      "Epoca 7/20, Loss: 0.038720\n",
      "Epoca 8/20, Loss: 0.035956\n",
      "Epoca 9/20, Loss: 0.034399\n",
      "Epoca 10/20, Loss: 0.032981\n",
      "Epoca 11/20, Loss: 0.031743\n",
      "Epoca 12/20, Loss: 0.030464\n",
      "Epoca 13/20, Loss: 0.029476\n",
      "Epoca 14/20, Loss: 0.028756\n",
      "Epoca 15/20, Loss: 0.028224\n",
      "Epoca 16/20, Loss: 0.027814\n",
      "Epoca 17/20, Loss: 0.027240\n",
      "Epoca 18/20, Loss: 0.026872\n",
      "Epoca 19/20, Loss: 0.026560\n",
      "Epoca 20/20, Loss: 0.026088\n",
      "Dimensioni dataset latente: X_train_latent: (5600, 8), X_test_latent: (1400, 8)\n",
      "Addestramento del modello quantum diffusion latente...\n",
      "Parametri totali del circuito: 510\n",
      "Inizio addestramento per 5 epoche...\n",
      "  Timestep 1/14, Loss: 0.967120\n",
      "  Timestep 2/14, Loss: 0.970018\n",
      "  Timestep 3/14, Loss: 0.971052\n",
      "  Timestep 4/14, Loss: 0.966976\n",
      "  Timestep 5/14, Loss: 0.952053\n",
      "  Timestep 6/14, Loss: 0.949053\n",
      "  Timestep 7/14, Loss: 0.947118\n",
      "  Timestep 8/14, Loss: 0.929185\n",
      "  Timestep 9/14, Loss: 0.933227\n",
      "  Timestep 10/14, Loss: 0.944590\n",
      "  Timestep 11/14, Loss: 0.946824\n",
      "  Timestep 12/14, Loss: 0.932533\n",
      "  Timestep 13/14, Loss: 0.926346\n",
      "  Timestep 14/14, Loss: 0.922443\n",
      "Epoca 1/5, Batch 1/88, Loss: 0.632678\n",
      "Epoca 1/5, Batch 2/88, Loss: 0.628108\n",
      "Epoca 1/5, Batch 3/88, Loss: 0.630199\n",
      "Epoca 1/5, Batch 4/88, Loss: 0.621560\n",
      "Epoca 1/5, Batch 5/88, Loss: 0.649325\n",
      "  Timestep 1/14, Loss: 0.536918\n",
      "  Timestep 2/14, Loss: 0.534757\n",
      "  Timestep 3/14, Loss: 0.538183\n",
      "  Timestep 4/14, Loss: 0.538651\n",
      "  Timestep 5/14, Loss: 0.528940\n",
      "  Timestep 6/14, Loss: 0.529983\n",
      "  Timestep 7/14, Loss: 0.538970\n",
      "  Timestep 8/14, Loss: 0.547053\n",
      "  Timestep 9/14, Loss: 0.550092\n",
      "  Timestep 10/14, Loss: 0.570757\n",
      "  Timestep 11/14, Loss: 0.567550\n",
      "  Timestep 12/14, Loss: 0.551109\n",
      "  Timestep 13/14, Loss: 0.553292\n",
      "  Timestep 14/14, Loss: 0.545290\n",
      "Epoca 1/5, Batch 6/88, Loss: 0.656639\n",
      "Epoca 1/5, Batch 7/88, Loss: 0.638911\n",
      "Epoca 1/5, Batch 8/88, Loss: 0.671854\n",
      "Epoca 1/5, Batch 9/88, Loss: 0.641543\n",
      "Epoca 1/5, Batch 10/88, Loss: 0.649930\n",
      "  Timestep 1/14, Loss: 0.503371\n",
      "  Timestep 2/14, Loss: 0.497486\n",
      "  Timestep 3/14, Loss: 0.497192\n",
      "  Timestep 4/14, Loss: 0.492785\n",
      "  Timestep 5/14, Loss: 0.502282\n",
      "  Timestep 6/14, Loss: 0.508370\n",
      "  Timestep 7/14, Loss: 0.509098\n",
      "  Timestep 8/14, Loss: 0.507133\n",
      "  Timestep 9/14, Loss: 0.498744\n",
      "  Timestep 10/14, Loss: 0.507430\n",
      "  Timestep 11/14, Loss: 0.488832\n",
      "  Timestep 12/14, Loss: 0.488204\n",
      "  Timestep 13/14, Loss: 0.455599\n",
      "  Timestep 14/14, Loss: 0.448841\n",
      "Epoca 1/5, Batch 11/88, Loss: 0.630021\n",
      "Epoca 1/5, Batch 12/88, Loss: 0.611264\n",
      "Epoca 1/5, Batch 13/88, Loss: 0.664758\n",
      "Epoca 1/5, Batch 14/88, Loss: 0.615717\n",
      "Epoca 1/5, Batch 15/88, Loss: 0.651094\n",
      "  Timestep 1/14, Loss: 0.742640\n",
      "  Timestep 2/14, Loss: 0.742667\n",
      "  Timestep 3/14, Loss: 0.738238\n",
      "  Timestep 4/14, Loss: 0.732794\n",
      "  Timestep 5/14, Loss: 0.731286\n",
      "  Timestep 6/14, Loss: 0.724110\n",
      "  Timestep 7/14, Loss: 0.736589\n",
      "  Timestep 8/14, Loss: 0.722689\n",
      "  Timestep 9/14, Loss: 0.729507\n",
      "  Timestep 10/14, Loss: 0.723131\n",
      "  Timestep 11/14, Loss: 0.731999\n",
      "  Timestep 12/14, Loss: 0.721488\n",
      "  Timestep 13/14, Loss: 0.641420\n",
      "  Timestep 14/14, Loss: 0.641963\n",
      "Epoca 1/5, Batch 16/88, Loss: 0.661718\n",
      "Epoca 1/5, Batch 17/88, Loss: 0.653840\n",
      "Epoca 1/5, Batch 18/88, Loss: 0.655844\n",
      "Epoca 1/5, Batch 19/88, Loss: 0.600900\n",
      "Epoca 1/5, Batch 20/88, Loss: 0.663416\n",
      "  Timestep 1/14, Loss: 0.870418\n",
      "  Timestep 2/14, Loss: 0.870559\n",
      "  Timestep 3/14, Loss: 0.849061\n",
      "  Timestep 4/14, Loss: 0.854919\n",
      "  Timestep 5/14, Loss: 0.870618\n",
      "  Timestep 6/14, Loss: 0.891928\n",
      "  Timestep 7/14, Loss: 0.893333\n",
      "  Timestep 8/14, Loss: 0.893603\n",
      "  Timestep 9/14, Loss: 0.906108\n",
      "  Timestep 10/14, Loss: 0.927742\n",
      "  Timestep 11/14, Loss: 0.938927\n",
      "  Timestep 12/14, Loss: 0.957175\n",
      "  Timestep 13/14, Loss: 0.963107\n",
      "  Timestep 14/14, Loss: 0.964036\n",
      "Epoca 1/5, Batch 21/88, Loss: 0.620457\n",
      "Epoca 1/5, Batch 22/88, Loss: 0.641348\n",
      "Epoca 1/5, Batch 23/88, Loss: 0.667070\n",
      "Epoca 1/5, Batch 24/88, Loss: 0.663185\n",
      "Epoca 1/5, Batch 25/88, Loss: 0.657676\n",
      "  Timestep 1/14, Loss: 0.519861\n",
      "  Timestep 2/14, Loss: 0.518867\n",
      "  Timestep 3/14, Loss: 0.519915\n",
      "  Timestep 4/14, Loss: 0.519920\n",
      "  Timestep 5/14, Loss: 0.518095\n",
      "  Timestep 6/14, Loss: 0.518504\n",
      "  Timestep 7/14, Loss: 0.492727\n",
      "  Timestep 8/14, Loss: 0.490645\n",
      "  Timestep 9/14, Loss: 0.499274\n",
      "  Timestep 10/14, Loss: 0.486013\n",
      "  Timestep 11/14, Loss: 0.477942\n",
      "  Timestep 12/14, Loss: 0.494706\n",
      "  Timestep 13/14, Loss: 0.478018\n",
      "  Timestep 14/14, Loss: 0.487345\n",
      "Epoca 1/5, Batch 26/88, Loss: 0.593834\n",
      "Epoca 1/5, Batch 27/88, Loss: 0.639786\n",
      "Epoca 1/5, Batch 28/88, Loss: 0.642170\n",
      "Epoca 1/5, Batch 29/88, Loss: 0.614042\n",
      "Epoca 1/5, Batch 30/88, Loss: 0.650015\n",
      "  Timestep 1/14, Loss: 0.722768\n",
      "  Timestep 2/14, Loss: 0.722159\n",
      "  Timestep 3/14, Loss: 0.730552\n",
      "  Timestep 4/14, Loss: 0.732984\n",
      "  Timestep 5/14, Loss: 0.732928\n",
      "  Timestep 6/14, Loss: 0.720505\n",
      "  Timestep 7/14, Loss: 0.729456\n",
      "  Timestep 8/14, Loss: 0.718077\n",
      "  Timestep 9/14, Loss: 0.725922\n",
      "  Timestep 10/14, Loss: 0.721042\n",
      "  Timestep 11/14, Loss: 0.727643\n",
      "  Timestep 12/14, Loss: 0.732289\n",
      "  Timestep 13/14, Loss: 0.771300\n",
      "  Timestep 14/14, Loss: 0.736718\n",
      "Epoca 1/5, Batch 31/88, Loss: 0.685137\n",
      "Epoca 1/5, Batch 32/88, Loss: 0.626274\n",
      "Epoca 1/5, Batch 33/88, Loss: 0.612651\n",
      "Epoca 1/5, Batch 34/88, Loss: 0.639169\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementazione di un Hybrid Quantum Diffusion Model.\n",
    "- Forward process: Autoencoder classico\n",
    "- Backward process: Circuito quantistico parametrizzato (PQC)\n",
    "\n",
    "Basato sull'architettura reverse-bottleneck descritta nel paper.\n",
    "Include correzioni per le dimensioni dei parametri e configurazioni \n",
    "basate sui risultati di simulazione presentati nel paper.\n",
    "\"\"\"\n",
    "\n",
    "# Definizione dei parametri del modello in base ai risultati di simulazione del paper\n",
    "latent_qubits = 3    # 3 qubit per il modello latente\n",
    "m_ancilla = 1        # Numero di qubit ancillari\n",
    "label_qubits = 4     # 4 qubit per codificare le 10 classi MNIST\n",
    "\n",
    "# Profondità circuito in base al paper\n",
    "latent_n_layers = 50 // 3  # 50 layer totali distribuiti su 3 blocchi\n",
    "\n",
    "# Parametri diffusione\n",
    "timesteps = 15     # Numero ridotto di passi temporali come indicato nel paper\n",
    "beta_min = 1e-4    # Minimo valore di beta per la schedule\n",
    "beta_max = 0.02    # Massimo valore di beta per la schedule\n",
    "latent_dim = 2**latent_qubits  # Dimensione per il modello latente\n",
    "\n",
    "# Definizione della schedule del forward process\n",
    "def get_beta_schedule(timesteps, beta_min, beta_max):\n",
    "    \"\"\"Crea la schedule dei beta per il processo di diffusione.\"\"\"\n",
    "    return np.linspace(beta_min, beta_max, timesteps)\n",
    "\n",
    "# Calcolo degli alpha dal beta schedule\n",
    "def compute_alpha(beta_schedule):\n",
    "    \"\"\"Calcola gli alpha e gli alpha cumulativi.\"\"\"\n",
    "    alpha = 1 - beta_schedule\n",
    "    alpha_cumprod = np.cumprod(alpha)\n",
    "    return alpha, alpha_cumprod\n",
    "\n",
    "beta_schedule = get_beta_schedule(timesteps, beta_min, beta_max)\n",
    "alpha, alpha_cumprod = compute_alpha(beta_schedule)\n",
    "\n",
    "# PARTE 1: COMPONENTI DEL FORWARD PROCESS CLASSICO (AUTOENCODER)\n",
    "\n",
    "class AutoencoderForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder classico per il forward process.\n",
    "    Implementa l'aggiunta di rumore gaussiano complesso ai dati.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, timesteps, alpha_cumprod):\n",
    "        super(AutoencoderForward, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.timesteps = timesteps\n",
    "        self.alpha_cumprod = alpha_cumprod\n",
    "    \n",
    "    def forward_step(self, x, t):\n",
    "        \"\"\"\n",
    "        Esegue un singolo passo del forward process aggiungendo rumore complesso.\n",
    "        \n",
    "        Args:\n",
    "            x: Dati di input\n",
    "            t: Timestep corrente\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Dati rumorosi al timestep t\n",
    "            noise: Rumore complesso aggiunto\n",
    "        \"\"\"\n",
    "        # Ottieni alpha per questo timestep\n",
    "        a_t = self.alpha_cumprod[t]\n",
    "        \n",
    "        # Genera rumore complesso\n",
    "        noise_real = torch.randn_like(x)\n",
    "        noise_imag = torch.randn_like(x)\n",
    "        noise = noise_real + 1j * noise_imag\n",
    "        \n",
    "        # Applica il rumore secondo l'equazione del paper\n",
    "        x_t = torch.sqrt(torch.tensor(a_t)) * x + torch.sqrt(torch.tensor(1 - a_t)) * noise\n",
    "        \n",
    "        return x_t, noise\n",
    "    \n",
    "    def forward(self, x_0):\n",
    "        \"\"\"\n",
    "        Esegue il processo completo di diffusione forward.\n",
    "        \n",
    "        Args:\n",
    "            x_0: Dati originali\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Lista di dati diffusi per ogni timestep\n",
    "        \"\"\"\n",
    "        x_t_sequence = []\n",
    "        x_t = x_0\n",
    "        \n",
    "        # Applica sequenzialmente i passi di diffusione\n",
    "        for t in range(self.timesteps):\n",
    "            x_t, _ = self.forward_step(x_t, t)\n",
    "            x_t_sequence.append(x_t)\n",
    "        \n",
    "        return x_t_sequence\n",
    "\n",
    "# PARTE 2: COMPONENTI DEL BACKWARD PROCESS QUANTISTICO (PQC)\n",
    "\n",
    "# Definizione del blocco unitario parametrizzato\n",
    "def unitary_block(params, wires, n_layers):\n",
    "    \"\"\"\n",
    "    Implementa un blocco unitario parametrizzato con layer di rotazione e entanglement.\n",
    "    \n",
    "    Args:\n",
    "        params: Parametri del blocco\n",
    "        wires: Qubit su cui applicare il blocco\n",
    "        n_layers: Numero di layer nel blocco\n",
    "    \"\"\"\n",
    "    n_wires = len(wires)\n",
    "    \n",
    "    # Layer di rotazioni singolo-qubit\n",
    "    for i, wire in enumerate(wires):\n",
    "        qml.RX(params[0, i], wires=wire)\n",
    "        qml.RY(params[1, i], wires=wire)\n",
    "        qml.RZ(params[2, i], wires=wire)\n",
    "    \n",
    "    # Layer di entanglement\n",
    "    for layer in range(n_layers):\n",
    "        # Entanglement con CNOT\n",
    "        for i in range(n_wires-1):\n",
    "            qml.CNOT(wires=[wires[i], wires[i+1]])\n",
    "        qml.CNOT(wires=[wires[n_wires-1], wires[0]])  # Circular entanglement\n",
    "        \n",
    "        # Rotazioni parametrizzate\n",
    "        for i, wire in enumerate(wires):\n",
    "            qml.RX(params[3+layer*3, i], wires=wire)\n",
    "            qml.RY(params[4+layer*3, i], wires=wire)\n",
    "            qml.RZ(params[5+layer*3, i], wires=wire)\n",
    "\n",
    "# Definizione dell'encoding dei dati classici in stati quantistici (amplitude encoding)\n",
    "def amplitude_encode(x, wires):\n",
    "    \"\"\"\n",
    "    Implementa l'amplitude encoding di un vettore classico in uno stato quantistico.\n",
    "    \n",
    "    Args:\n",
    "        x: Vettore di dati classici (può essere complesso)\n",
    "        wires: Qubit su cui codificare l'informazione\n",
    "    \"\"\"\n",
    "    # Normalizzazione dell'ampiezza\n",
    "    x_norm = x / np.sqrt(np.sum(np.abs(x)**2))\n",
    "    qml.AmplitudeEmbedding(x_norm, wires=wires, normalize=True)\n",
    "\n",
    "# PARTE 3: MODELLO COMPLETO QUANTUM DIFFUSION\n",
    "\n",
    "# Creazione del dispositivo quantistico simulato \n",
    "def create_device(n_qubits, m_ancilla, extra_qubits=0):\n",
    "    \"\"\"\n",
    "    Crea un dispositivo quantistico con il numero corretto di qubit.\n",
    "    Utilizza default.mixed che supporta meglio le misurazioni intermedie.\n",
    "    \n",
    "    Args:\n",
    "        n_qubits: Numero di qubit per i dati\n",
    "        m_ancilla: Numero di qubit ancillari\n",
    "        extra_qubits: Qubit aggiuntivi (per etichette, ecc.)\n",
    "        \n",
    "    Returns:\n",
    "        Dispositivo quantistico PennyLane\n",
    "    \"\"\"\n",
    "    total_qubits = n_qubits + m_ancilla + extra_qubits\n",
    "    return qml.device('default.mixed', wires=total_qubits)\n",
    "\n",
    "# Definizione del circuito quantistico parametrizzato per il reverse-bottleneck\n",
    "def create_reverse_bottleneck_pqc(dev, n_qubits, m_ancilla, n_layers, use_labels=False, n_label_qubits=0):\n",
    "    \"\"\"\n",
    "    Crea un circuito PQC reverse-bottleneck per il quantum diffusion.\n",
    "    \n",
    "    Args:\n",
    "        dev: Dispositivo quantistico\n",
    "        n_qubits: Numero di qubit per i dati\n",
    "        m_ancilla: Numero di qubit ancillari\n",
    "        n_layers: Numero di layer nel circuito\n",
    "        use_labels: Se True, aggiunge qubit per le etichette\n",
    "        n_label_qubits: Numero di qubit per le etichette\n",
    "        \n",
    "    Returns:\n",
    "        Funzione QNode per il circuito PQC\n",
    "    \"\"\"\n",
    "    @qml.qnode(dev, diff_method=\"parameter-shift\")\n",
    "    def circuit(x_t, params, t, label=None):\n",
    "        # Calcola il numero di parametri necessari per blocco\n",
    "        params_per_block = 3 + n_layers * 3\n",
    "        \n",
    "        # Determina il numero di qubit effettivi per ciascun blocco\n",
    "        n_block1_qubits = n_qubits + (n_label_qubits if use_labels else 0)\n",
    "        n_block2_qubits = n_qubits + m_ancilla + (n_label_qubits if use_labels else 0)\n",
    "        n_block3_qubits = n_qubits\n",
    "        \n",
    "        # Calcola i parametri necessari per ciascun blocco\n",
    "        block1_size = params_per_block * n_block1_qubits\n",
    "        block2_size = params_per_block * n_block2_qubits\n",
    "        block3_size = params_per_block * n_block3_qubits\n",
    "        \n",
    "        # Verifica che il numero di parametri sia corretto\n",
    "        total_size = block1_size + block2_size + block3_size\n",
    "        if params.size != total_size:\n",
    "            raise ValueError(f\"Numero di parametri errato. Atteso: {total_size}, Ricevuto: {params.size}\")\n",
    "        \n",
    "        # Ridimensiona correttamente i parametri per ciascun blocco\n",
    "        block1_params = params[:block1_size].reshape(params_per_block, n_block1_qubits)\n",
    "        block2_params = params[block1_size:block1_size+block2_size].reshape(params_per_block, n_block2_qubits)\n",
    "        block3_params = params[block1_size+block2_size:].reshape(params_per_block, n_block3_qubits)\n",
    "        \n",
    "        # Amplitude encoding dello stato rumoroso\n",
    "        amplitude_encode(x_t, wires=range(n_qubits))\n",
    "        \n",
    "        # Se stiamo usando le etichette, prepara gli stati per le etichette\n",
    "        if use_labels and label is not None:\n",
    "            # Converti l'etichetta in rappresentazione binaria\n",
    "            binary_label = [int(b) for b in format(label, f'0{n_label_qubits}b')]\n",
    "            \n",
    "            # Imposta i qubit delle etichette agli stati corretti\n",
    "            for i, bit in enumerate(binary_label):\n",
    "                if bit == 1:\n",
    "                    qml.PauliX(wires=n_qubits + m_ancilla + i)\n",
    "        \n",
    "        # Determina i fili (wires) per ciascun blocco\n",
    "        wires_block1 = list(range(n_qubits))\n",
    "        if use_labels:\n",
    "            # Aggiungi i qubit delle etichette\n",
    "            wires_block1 += list(range(n_qubits + m_ancilla, n_qubits + m_ancilla + n_label_qubits))\n",
    "        \n",
    "        # Blocco unitario 1\n",
    "        unitary_block(block1_params, wires_block1, n_layers)\n",
    "        \n",
    "        # Blocco unitario 2 (con qubit ancillari)\n",
    "        wires_block2 = list(range(n_qubits + m_ancilla))\n",
    "        if use_labels:\n",
    "            # Aggiungi i qubit delle etichette\n",
    "            wires_block2 += list(range(n_qubits + m_ancilla, n_qubits + m_ancilla + n_label_qubits))\n",
    "        \n",
    "        unitary_block(block2_params, wires_block2, n_layers)\n",
    "        \n",
    "        # Misurazione del qubit ancillare\n",
    "        if m_ancilla > 0:\n",
    "            for i in range(n_qubits, n_qubits + m_ancilla):\n",
    "                qml.measure(wires=i)\n",
    "        \n",
    "        # Se stiamo usando le etichette, misura anche i qubit delle etichette\n",
    "        if use_labels:\n",
    "            for i in range(n_label_qubits):\n",
    "                qml.measure(wires=n_qubits + m_ancilla + i)\n",
    "        \n",
    "        # Blocco unitario 3 (solo sui qubit di dati)\n",
    "        unitary_block(block3_params, range(n_qubits), n_layers)\n",
    "        \n",
    "        # Restituisci l'ampiezza dello stato risultante\n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "# Definizione della funzione di loss (infidelity)\n",
    "def create_infidelity_loss_fn(reverse_bottleneck_pqc, n_qubits, use_labels=False):\n",
    "    \"\"\"\n",
    "    Crea una funzione di loss basata sull'infidelity tra lo stato target e la previsione.\n",
    "    \n",
    "    Args:\n",
    "        reverse_bottleneck_pqc: Funzione del circuito PQC\n",
    "        n_qubits: Numero di qubit per i dati\n",
    "        use_labels: Se True, considera le etichette nella loss\n",
    "        \n",
    "    Returns:\n",
    "        Funzione di loss\n",
    "    \"\"\"\n",
    "    def infidelity_loss(params, x_t, x_t_minus_1, t, label=None):\n",
    "        \"\"\"\n",
    "        Calcola l'infidelity loss.\n",
    "        \n",
    "        Args:\n",
    "            params: Parametri del circuito\n",
    "            x_t: Stato rumoroso al timestep t\n",
    "            x_t_minus_1: Stato target (meno rumoroso) al timestep t-1\n",
    "            t: Timestep corrente\n",
    "            label: Etichetta per il condizionamento (opzionale)\n",
    "            \n",
    "        Returns:\n",
    "            Infidelity loss (1 - fidelity)\n",
    "        \"\"\"\n",
    "        # Assicurati che tutto sia in formato NumPy\n",
    "        if torch.is_tensor(params):\n",
    "            params = params.detach().cpu().numpy()\n",
    "        if torch.is_tensor(x_t):\n",
    "            x_t = x_t.detach().cpu().numpy()\n",
    "        if torch.is_tensor(x_t_minus_1):\n",
    "            x_t_minus_1 = x_t_minus_1.detach().cpu().numpy()\n",
    "\n",
    "        try:\n",
    "            # Ottieni lo stato previsto dal circuito\n",
    "            if use_labels and label is not None:\n",
    "                circuit_output = reverse_bottleneck_pqc(x_t, params, t, label)\n",
    "            else:\n",
    "                circuit_output = reverse_bottleneck_pqc(x_t, params, t)\n",
    "            \n",
    "            # Per il dispositivo default.qubit, il risultato potrebbe essere diverso\n",
    "            # Controlliamo se abbiamo ottenuto una densità di probabilità o uno stato\n",
    "            if len(circuit_output.shape) == 1:  # Abbiamo ottenuto le probabilità\n",
    "                # Recupera le ampiezze come radice quadrata delle probabilità (fase uguale a 0)\n",
    "                predicted_amplitudes = np.sqrt(circuit_output)\n",
    "            else:  # Abbiamo ottenuto lo stato completo o una matrice densità\n",
    "                # Estrazione della parte rilevante dello stato (escludendo ancilla e etichette)\n",
    "                predicted_amplitudes = circuit_output[:2**n_qubits]\n",
    "            \n",
    "            # Normalizzazione dell'ampiezza di x_t_minus_1\n",
    "            x_target_norm = x_t_minus_1 / np.sqrt(np.sum(np.abs(x_t_minus_1)**2))\n",
    "            \n",
    "            # La fidelity è |<ψ|φ>|²\n",
    "            fidelity = np.abs(np.vdot(predicted_amplitudes, x_target_norm))**2\n",
    "            \n",
    "            # Infidelity = 1 - fidelity\n",
    "            return 1 - fidelity\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel calcolo dell'infidelity loss: {str(e)}\")\n",
    "            return 1.0\n",
    "    \n",
    "    return infidelity_loss\n",
    "\n",
    "# Calcolo dei parametri necessari per il circuito\n",
    "def calculate_circuit_parameters(n_qubits, m_ancilla, n_layers, use_labels=False, n_label_qubits=0):\n",
    "    \"\"\"\n",
    "    Calcola il numero e la struttura dei parametri necessari per il circuito.\n",
    "    \n",
    "    Args:\n",
    "        n_qubits: Numero di qubit per i dati\n",
    "        m_ancilla: Numero di qubit ancillari\n",
    "        n_layers: Numero di layer nel circuito\n",
    "        use_labels: Se True, considera le etichette\n",
    "        n_label_qubits: Numero di qubit per le etichette\n",
    "        \n",
    "    Returns:\n",
    "        total_params: Numero totale di parametri\n",
    "        params_structure: Dizionario con la struttura dei parametri\n",
    "    \"\"\"\n",
    "    params_per_block = 3 + n_layers * 3\n",
    "    \n",
    "    # Determina il numero di qubit effettivi per ciascun blocco\n",
    "    n_block1_qubits = n_qubits + (n_label_qubits if use_labels else 0)\n",
    "    n_block2_qubits = n_qubits + m_ancilla + (n_label_qubits if use_labels else 0)\n",
    "    n_block3_qubits = n_qubits\n",
    "    \n",
    "    # Calcola i parametri necessari per ciascun blocco\n",
    "    block1_size = params_per_block * n_block1_qubits\n",
    "    block2_size = params_per_block * n_block2_qubits\n",
    "    block3_size = params_per_block * n_block3_qubits\n",
    "    \n",
    "    total_params = block1_size + block2_size + block3_size\n",
    "    \n",
    "    params_structure = {\n",
    "        'total': total_params,\n",
    "        'block1': {\n",
    "            'size': block1_size,\n",
    "            'shape': (params_per_block, n_block1_qubits)\n",
    "        },\n",
    "        'block2': {\n",
    "            'size': block2_size,\n",
    "            'shape': (params_per_block, n_block2_qubits)\n",
    "        },\n",
    "        'block3': {\n",
    "            'size': block3_size,\n",
    "            'shape': (params_per_block, n_block3_qubits)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return total_params, params_structure\n",
    "\n",
    "# PARTE 4: ADDESTRAMENTO DEL MODELLO\n",
    "\n",
    "def train_quantum_diffusion(X_train, y_train=None, n_qubits=8, m_ancilla=1, n_layers=50, \n",
    "                          use_labels=False, n_label_qubits=0, n_epochs=20, batch_size=32, lr=0.01):\n",
    "    \"\"\"\n",
    "    Addestra il modello di quantum diffusion.\n",
    "    \"\"\"\n",
    "    # Inizializzazione del forward process\n",
    "    forward_process = AutoencoderForward(X_train.shape[1], timesteps, alpha_cumprod)\n",
    "    \n",
    "    # Calcolo dei parametri necessari per il circuito quantistico\n",
    "    total_params, params_structure = calculate_circuit_parameters(\n",
    "        n_qubits, m_ancilla, n_layers, use_labels, n_label_qubits)\n",
    "    \n",
    "    print(f\"Parametri totali del circuito: {total_params}\")\n",
    "    \n",
    "    # Creazione del dispositivo quantistico\n",
    "    extra_qubits = n_label_qubits if use_labels else 0\n",
    "    dev = qml.device('default.qubit', wires=n_qubits + m_ancilla + extra_qubits)\n",
    "    \n",
    "    # Creazione del circuito e della funzione di loss\n",
    "    circuit = create_reverse_bottleneck_pqc(dev, n_qubits, m_ancilla, n_layers, use_labels, n_label_qubits)\n",
    "    loss_fn = create_infidelity_loss_fn(circuit, n_qubits, use_labels)\n",
    "    \n",
    "    # Inizializzazione dei parametri del circuito\n",
    "    np.random.seed(42)\n",
    "    params = np.random.uniform(0, 2*np.pi, total_params)\n",
    "    \n",
    "    # AGGIUNTA: Crea tensore PyTorch per i parametri con requires_grad=True\n",
    "    params_tensor = torch.tensor(params, requires_grad=True, dtype=torch.float64)\n",
    "    \n",
    "    # AGGIUNTA: Crea ottimizzatore\n",
    "    optimizer = torch.optim.Adam([params_tensor], lr=lr)\n",
    "    \n",
    "    # Conversione del dataset in tensori PyTorch\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.complex128)\n",
    "    \n",
    "    # Verifica della compatibilità delle dimensioni per il training condizionato\n",
    "    if use_labels and y_train is not None:\n",
    "        if len(y_train) != len(X_train_tensor):\n",
    "            min_len = min(len(y_train), len(X_train_tensor))\n",
    "            X_train_tensor = X_train_tensor[:min_len]\n",
    "            y_train = y_train[:min_len]\n",
    "            print(f\"Dataset ridimensionato a {min_len} esempi\")\n",
    "    \n",
    "    # Tracciamento della loss\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Inizio addestramento per {n_epochs} epoche...\")\n",
    "    \n",
    "    # Gestione degli indici del dataset\n",
    "    indices = np.arange(len(X_train_tensor))\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            # Shuffle del dataset\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # Addestramento su mini-batch\n",
    "            num_batches = math.ceil(len(X_train_tensor) / batch_size)\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                # Calcola gli indici per questo batch\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, len(indices))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                # Preparazione del batch\n",
    "                batch_X = X_train_tensor[batch_indices]\n",
    "                batch_y = y_train[batch_indices] if use_labels and y_train is not None else None\n",
    "                \n",
    "                # MODIFICA: Reset del gradiente all'inizio di ogni batch\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Calcolo del batch loss\n",
    "                batch_loss = 0\n",
    "                successful_samples = 0\n",
    "                \n",
    "                for idx in range(len(batch_X)):\n",
    "                    x_0 = batch_X[idx]\n",
    "                    \n",
    "                    # Applicazione del forward process\n",
    "                    x_t_sequence = forward_process(x_0)\n",
    "                    \n",
    "                    # Label per condizionamento (se applicabile)\n",
    "                    label = int(batch_y[idx]) if use_labels and batch_y is not None else None\n",
    "                    \n",
    "                    # Calcolo della loss per ogni timestep\n",
    "                    sample_loss = 0\n",
    "                    timestep_count = 0\n",
    "                    \n",
    "                    for t in range(1, timesteps):\n",
    "                        # Converti in NumPy\n",
    "                        x_t = x_t_sequence[t].detach().cpu().numpy()\n",
    "                        x_t_minus_1 = x_t_sequence[t-1].detach().cpu().numpy()\n",
    "                        \n",
    "                        # Calcola la loss usando i parametri come tensore PyTorch\n",
    "                        current_loss = loss_fn(params_tensor.detach().cpu().numpy(), x_t, x_t_minus_1, t, label)\n",
    "                        sample_loss += current_loss\n",
    "                        timestep_count += 1\n",
    "                        \n",
    "                        # Stampa informazioni sui timestep per debug\n",
    "                        if idx == 0 and i % 5 == 0:  # Stampa solo per il primo campione e ogni 5 batch\n",
    "                            print(f\"  Timestep {t}/{timesteps-1}, Loss: {current_loss:.6f}\")\n",
    "                    \n",
    "                    # Media della loss sul campione\n",
    "                    if timestep_count > 0:\n",
    "                        sample_loss /= timestep_count\n",
    "                        batch_loss += sample_loss\n",
    "                        successful_samples += 1\n",
    "                \n",
    "                # Media della loss sul batch\n",
    "                if successful_samples > 0:\n",
    "                    batch_loss /= successful_samples\n",
    "                    \n",
    "                    # AGGIUNTA: Crea un tensore per la loss che mantiene il grafo computazionale\n",
    "                    batch_loss_tensor = torch.tensor(batch_loss, requires_grad=True)\n",
    "                    \n",
    "                    # AGGIUNTA: Backpropagation\n",
    "                    batch_loss_tensor.backward()\n",
    "                    \n",
    "                    # AGGIUNTA: Aggiornamento dei parametri\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += batch_loss\n",
    "                    print(f\"Epoca {epoch+1}/{n_epochs}, Batch {i+1}/{num_batches}, Loss: {batch_loss:.6f}\")\n",
    "            \n",
    "            # Media della loss sull'epoch\n",
    "            if num_batches > 0:\n",
    "                epoch_loss /= num_batches\n",
    "                losses.append(epoch_loss)\n",
    "                print(f\"Epoca {epoch+1}/{n_epochs}, Loss media: {epoch_loss:.6f}\")\n",
    "                \n",
    "                # AGGIUNTA: Salva i parametri correnti in formato NumPy\n",
    "                params = params_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'addestramento: {str(e)}\")\n",
    "        traceback.print_exc()  # Aggiunta per stampare il traceback completo\n",
    "        \n",
    "        # Se non ci sono loss registrate, aggiungi un valore di default\n",
    "        if len(losses) == 0:\n",
    "            losses.append(1.0)\n",
    "            \n",
    "        # Aggiornamento dei parametri finali\n",
    "        params = params_tensor.detach().cpu().numpy()\n",
    "\n",
    "    return params, losses\n",
    "\n",
    "# PARTE 5: SAMPLING DAL MODELLO ADDESTRATO\n",
    "\n",
    "def sample_from_model(params, n_samples, n_qubits, m_ancilla, n_layers, use_labels=False, \n",
    "                      n_label_qubits=0, labels=None):\n",
    "    \"\"\"\n",
    "    Genera campioni dal modello di diffusione quantistica addestrato.\n",
    "    \n",
    "    Args:\n",
    "        params: Parametri addestrati del circuito\n",
    "        n_samples: Numero di campioni da generare\n",
    "        n_qubits: Numero di qubit per i dati\n",
    "        m_ancilla: Numero di qubit ancillari\n",
    "        n_layers: Numero di layer nel circuito\n",
    "        use_labels: Se True, usa il modello condizionato\n",
    "        n_label_qubits: Numero di qubit per le etichette\n",
    "        labels: Etichette per il condizionamento (se use_labels=True)\n",
    "        \n",
    "    Returns:\n",
    "        samples: Campioni generati\n",
    "    \"\"\"\n",
    "    # Creazione del dispositivo e del circuito\n",
    "    extra_qubits = n_label_qubits if use_labels else 0\n",
    "    dev = create_device(n_qubits, m_ancilla, extra_qubits)\n",
    "    circuit = create_reverse_bottleneck_pqc(dev, n_qubits, m_ancilla, n_layers, use_labels, n_label_qubits)\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    # Se non ci sono etichette fornite ma use_labels è True, genera etichette casuali\n",
    "    if use_labels and labels is None:\n",
    "        labels = np.random.randint(0, 2**n_label_qubits, size=n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Inizia con rumore puro (gaussiano complesso)\n",
    "        feature_dim = 2**n_qubits\n",
    "        noise_real = np.random.normal(0, 1, size=feature_dim)\n",
    "        noise_imag = np.random.normal(0, 1, size=feature_dim)\n",
    "        x_T = noise_real + 1j * noise_imag\n",
    "        \n",
    "        # Normalizzazione\n",
    "        x_T = x_T / np.sqrt(np.sum(np.abs(x_T)**2))\n",
    "        \n",
    "        # Etichetta per il condizionamento (se applicabile)\n",
    "        label = int(labels[i]) if use_labels and labels is not None else None\n",
    "        \n",
    "        # Processo di denoising inverso\n",
    "        x_t = x_T\n",
    "        \n",
    "        for t in reversed(range(timesteps)):\n",
    "            # Applica il circuito quantistico per rimuovere il rumore\n",
    "            if use_labels and label is not None:\n",
    "                state = circuit(x_t, params, t, label)\n",
    "            else:\n",
    "                state = circuit(x_t, params, t)\n",
    "            \n",
    "            # Estrai le ampiezze rilevanti\n",
    "            x_t = state[:2**n_qubits]\n",
    "            \n",
    "            # Normalizzazione\n",
    "            x_t = x_t / np.sqrt(np.sum(np.abs(x_t)**2))\n",
    "        \n",
    "        # Converti in valori assoluti come indicato nel paper\n",
    "        sample = np.abs(x_t)\n",
    "        samples.append(sample)\n",
    "    \n",
    "    return np.array(samples)\n",
    "\n",
    "# PARTE 6: MODELLO LATENTE (HYBRID CLASSICAL-QUANTUM)\n",
    "\n",
    "class ClassicalAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder classico per la riduzione della dimensionalità.\n",
    "    Usato nella variante latente del quantum diffusion model.\n",
    "    \n",
    "    Questo componente implementa la parte \"Latent models\" descritta nella sezione 3.3 del paper,\n",
    "    dove un autoencoder classico pre-addestrato viene utilizzato per ridurre la dimensionalità\n",
    "    dei dati prima di applicare il quantum diffusion model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(ClassicalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim),\n",
    "            nn.Sigmoid()  # Per immagini normalizzate tra 0 e 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        z = self.encoder(x)\n",
    "        # Decoding\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Codifica i dati nello spazio latente.\"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decodifica dallo spazio latente allo spazio originale.\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "def train_classical_autoencoder(X_train, latent_dim, n_epochs=50, batch_size=64, lr=0.001):\n",
    "    \"\"\"\n",
    "    Addestra un autoencoder classico per la riduzione della dimensionalità.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Dataset di addestramento\n",
    "        latent_dim: Dimensione dello spazio latente\n",
    "        n_epochs: Numero di epoche\n",
    "        batch_size: Dimensione del batch\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        model: Autoencoder addestrato\n",
    "    \"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = ClassicalAutoencoder(input_dim, latent_dim)\n",
    "    \n",
    "    # Criterio di loss e ottimizzatore\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Conversione a tensori PyTorch\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    \n",
    "    # Dataset e DataLoader\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, X_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Inizio addestramento dell'autoencoder per {n_epochs} epoche...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data, targets in dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass e ottimizzazione\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Stampa della loss\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoca {epoch+1}/{n_epochs}, Loss: {epoch_loss:.6f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# PARTE 7: METRICHE DI VALUTAZIONE\n",
    "# Come descritto nella sezione 3.4 \"Model evaluation\" del paper\n",
    "\n",
    "def calculate_roc_auc(samples, true_labels, n_classes=10):\n",
    "    \"\"\"\n",
    "    Calcola lo score ROC-AUC per valutare le prestazioni di condizionamento.\n",
    "    \n",
    "    Args:\n",
    "        samples: Campioni generati\n",
    "        true_labels: Etichette vere dei campioni\n",
    "        n_classes: Numero di classi\n",
    "        \n",
    "    Returns:\n",
    "        roc_auc_scores: Score ROC-AUC per ogni classe\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    # Addestramento dei classificatori\n",
    "    classifiers = []\n",
    "    for i in range(n_classes):\n",
    "        # Creazione di un dataset binario per ciascuna classe\n",
    "        binary_labels = (true_labels == i).astype(int)\n",
    "        \n",
    "        # Addestramento del classificatore\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "        clf.fit(samples, binary_labels)\n",
    "        classifiers.append(clf)\n",
    "    \n",
    "    # Calcolo dello score ROC-AUC per ogni classe\n",
    "    roc_auc_scores = []\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        # Predizione delle probabilità per la classe i\n",
    "        pred_probs = clf.predict_proba(samples)[:, 1]\n",
    "        \n",
    "        # Creazione di etichette binarie per la classe i\n",
    "        binary_labels = (true_labels == i).astype(int)\n",
    "        \n",
    "        # Calcolo dello score ROC-AUC\n",
    "        auc_score = roc_auc_score(binary_labels, pred_probs)\n",
    "        roc_auc_scores.append(auc_score)\n",
    "    \n",
    "    return roc_auc_scores\n",
    "\n",
    "def calculate_frechet_inception_distance(real_samples, generated_samples):\n",
    "    \"\"\"\n",
    "    Calcola la Fréchet Inception Distance (FID) tra campioni reali e generati.\n",
    "    \n",
    "    Args:\n",
    "        real_samples: Campioni reali\n",
    "        generated_samples: Campioni generati\n",
    "        \n",
    "    Returns:\n",
    "        fid: Fréchet Inception Distance\n",
    "    \"\"\"\n",
    "    from scipy import linalg\n",
    "    \n",
    "    # Calcola media e covarianza per i campioni reali\n",
    "    mu1 = np.mean(real_samples, axis=0)\n",
    "    sigma1 = np.cov(real_samples, rowvar=False)\n",
    "    \n",
    "    # Calcola media e covarianza per i campioni generati\n",
    "    mu2 = np.mean(generated_samples, axis=0)\n",
    "    sigma2 = np.cov(generated_samples, rowvar=False)\n",
    "    \n",
    "    # Calcola la FID\n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    # Aggiunta di una piccola costante per evitare problemi numerici\n",
    "    sigma1 = sigma1 + np.eye(sigma1.shape[0]) * 1e-6\n",
    "    sigma2 = sigma2 + np.eye(sigma2.shape[0]) * 1e-6\n",
    "    \n",
    "    # Calcolo della media geometrica\n",
    "    covmean = linalg.sqrtm(sigma1.dot(sigma2))\n",
    "    \n",
    "    # Verifica se ci sono valori immaginari numericamente insignificanti\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    # Calcolo della FID\n",
    "    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)\n",
    "    \n",
    "    return fid\n",
    "\n",
    "def calculate_wasserstein_distance_gaussian_mixture(real_samples, generated_samples, n_components=10):\n",
    "    \"\"\"\n",
    "    Calcola la 2-Wasserstein distance per Gaussian mixture models (WaM).\n",
    "    Come specificato nella sezione 3.4 del paper, questa metrica è utile in quanto il FID\n",
    "    può soffrire di problemi con immagini piccole e in scala di grigi.\n",
    "    \n",
    "    Args:\n",
    "        real_samples: Campioni reali\n",
    "        generated_samples: Campioni generati\n",
    "        n_components: Numero di componenti nel mixture model\n",
    "        \n",
    "    Returns:\n",
    "        wam: 2-Wasserstein distance\n",
    "    \"\"\"\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    # Addestra il GMM sui campioni reali\n",
    "    gmm_real = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm_real.fit(real_samples)\n",
    "    \n",
    "    # Addestra il GMM sui campioni generati\n",
    "    gmm_gen = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm_gen.fit(generated_samples)\n",
    "    \n",
    "    # Estrai i parametri dei modelli\n",
    "    means_real = gmm_real.means_\n",
    "    covs_real = gmm_real.covariances_\n",
    "    weights_real = gmm_real.weights_\n",
    "    \n",
    "    means_gen = gmm_gen.means_\n",
    "    covs_gen = gmm_gen.covariances_\n",
    "    weights_gen = gmm_gen.weights_\n",
    "    \n",
    "    # Calcolo della WaM (approssimazione)\n",
    "    wam = 0\n",
    "    for i in range(n_components):\n",
    "        for j in range(n_components):\n",
    "            # Calcolo della 2-Wasserstein distance tra due distribuzioni gaussiane\n",
    "            diff_means = means_real[i] - means_gen[j]\n",
    "            term1 = np.sum(diff_means**2)\n",
    "            \n",
    "            # Calcolo della traccia\n",
    "            cov_real = covs_real[i]\n",
    "            cov_gen = covs_gen[j]\n",
    "            \n",
    "            # Calcolo della media geometrica (approssimazione)\n",
    "            cov_mean = (cov_real + cov_gen) / 2\n",
    "            \n",
    "            term2 = np.trace(cov_real) + np.trace(cov_gen) - 2 * np.trace(cov_mean)\n",
    "            \n",
    "            # Calcolo della distanza pesata\n",
    "            wam += weights_real[i] * weights_gen[j] * (term1 + term2)\n",
    "    \n",
    "    return wam\n",
    "\n",
    "# PARTE 8: PREPROCESSING DEL DATASET MNIST\n",
    "\n",
    "def preprocess_mnist_for_latent_model(latent_dim=8, fraction=1.0):\n",
    "    \"\"\"\n",
    "    Preprocessa il dataset MNIST per il modello latente:\n",
    "    - Normalizza i valori tra 0 e 1\n",
    "    - Addestra un autoencoder per ridurre la dimensionalità a latent_dim\n",
    "    - Opzionalmente utilizza solo una frazione del dataset\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimensione dello spazio latente\n",
    "        fraction: Frazione del dataset da utilizzare (1.0 = tutto)\n",
    "        \n",
    "    Returns:\n",
    "        X_train_latent: Dati di training nello spazio latente\n",
    "        X_test_latent: Dati di test nello spazio latente\n",
    "        y_train: Etichette di training\n",
    "        y_test: Etichette di test\n",
    "        autoencoder: Autoencoder addestrato per codifica/decodifica\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # Carica il dataset MNIST\n",
    "    print(\"Caricamento del dataset MNIST...\")\n",
    "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto')\n",
    "    X = X.astype('float32')\n",
    "    y = y.astype('int')\n",
    "    \n",
    "    # Utilizza solo una frazione del dataset se richiesto\n",
    "    if fraction < 1.0:\n",
    "        X_subset_indices = np.random.choice(len(X), size=int(len(X) * fraction), replace=False)\n",
    "        # Il DataFrame richiede .iloc o .loc per l'indicizzazione con array di indici\n",
    "        if hasattr(X, 'iloc'):\n",
    "            X = X.iloc[X_subset_indices]\n",
    "            y = y.iloc[X_subset_indices] if hasattr(y, 'iloc') else y[X_subset_indices]\n",
    "        else:\n",
    "            # Se X è un array NumPy, possiamo indicizzarlo direttamente\n",
    "            X = X[X_subset_indices]\n",
    "            y = y[X_subset_indices]\n",
    "        print(f\"Utilizzando il {fraction*100:.1f}% del dataset: {len(X)} esempi\")\n",
    "    \n",
    "    # Normalizza i dati tra 0 e 1\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Dividi in training e test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Addestra l'autoencoder per la riduzione della dimensionalità\n",
    "    print(f\"Addestramento dell'autoencoder per ridurre la dimensionalità a {latent_dim}...\")\n",
    "    autoencoder = train_classical_autoencoder(X_train, latent_dim, n_epochs=20)\n",
    "    \n",
    "    # Converti i dati nello spazio latente\n",
    "    with torch.no_grad():\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        \n",
    "        X_train_latent = autoencoder.encode(X_train_tensor).numpy()\n",
    "        X_test_latent = autoencoder.encode(X_test_tensor).numpy()\n",
    "    \n",
    "    print(f\"Dimensioni dataset latente: X_train_latent: {X_train_latent.shape}, X_test_latent: {X_test_latent.shape}\")\n",
    "    \n",
    "    return X_train_latent, X_test_latent, y_train, y_test, autoencoder\n",
    "\n",
    "# PARTE 9: VISUALIZZAZIONE DEI RISULTATI\n",
    "\n",
    "def plot_generated_samples(samples_uncond=None, samples_cond=None, n_images=25):\n",
    "    \"\"\"\n",
    "    Visualizza i campioni generati dal modello.\n",
    "    \n",
    "    Args:\n",
    "        samples_uncond: Campioni generati dal modello non condizionato\n",
    "        samples_cond: Campioni generati dal modello condizionato\n",
    "        n_images: Numero di immagini da visualizzare\n",
    "    \"\"\"\n",
    "    import math\n",
    "    \n",
    "    # Calcola righe e colonne\n",
    "    imgs_per_row = 5\n",
    "    rows_per_model = math.ceil(min(n_images, 25) / imgs_per_row)\n",
    "    \n",
    "    # Visualizza i campioni non condizionati\n",
    "    if samples_uncond is not None:\n",
    "        n_to_show = min(n_images, len(samples_uncond))\n",
    "        fig_uncond, axes_uncond = plt.subplots(rows_per_model, imgs_per_row, \n",
    "                                              figsize=(15, 4 * rows_per_model))\n",
    "        \n",
    "        # Gestisci il caso di un singolo asse\n",
    "        if rows_per_model == 1 and imgs_per_row == 1:\n",
    "            axes_uncond = np.array([axes_uncond])\n",
    "        if rows_per_model == 1:\n",
    "            axes_uncond = np.expand_dims(axes_uncond, axis=0)\n",
    "            \n",
    "        # Visualizza le immagini\n",
    "        for i in range(rows_per_model):\n",
    "            for j in range(imgs_per_row):\n",
    "                idx = i * imgs_per_row + j\n",
    "                if idx < n_to_show:\n",
    "                    sample_idx = idx % len(samples_uncond)\n",
    "                    ax = axes_uncond[i, j] if rows_per_model > 1 else axes_uncond[j]\n",
    "                    ax.imshow(samples_uncond[sample_idx].reshape(28, 28), cmap='gray')\n",
    "                    ax.axis('off')\n",
    "                else:\n",
    "                    if rows_per_model > 1:\n",
    "                        axes_uncond[i, j].axis('off')\n",
    "                    else:\n",
    "                        axes_uncond[j].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Campioni dal modello non condizionato\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "    \n",
    "    # Visualizza i campioni condizionati\n",
    "    if samples_cond is not None:\n",
    "        n_to_show = min(n_images, len(samples_cond))\n",
    "        fig_cond, axes_cond = plt.subplots(rows_per_model, imgs_per_row, \n",
    "                                          figsize=(15, 4 * rows_per_model))\n",
    "        \n",
    "        # Gestisci il caso di un singolo asse\n",
    "        if rows_per_model == 1 and imgs_per_row == 1:\n",
    "            axes_cond = np.array([axes_cond])\n",
    "        if rows_per_model == 1:\n",
    "            axes_cond = np.expand_dims(axes_cond, axis=0)\n",
    "            \n",
    "        # Visualizza le immagini\n",
    "        for i in range(rows_per_model):\n",
    "            for j in range(imgs_per_row):\n",
    "                idx = i * imgs_per_row + j\n",
    "                if idx < n_to_show:\n",
    "                    sample_idx = idx % len(samples_cond)\n",
    "                    ax = axes_cond[i, j] if rows_per_model > 1 else axes_cond[j]\n",
    "                    ax.imshow(samples_cond[sample_idx].reshape(28, 28), cmap='gray')\n",
    "                    ax.axis('off')\n",
    "                else:\n",
    "                    if rows_per_model > 1:\n",
    "                        axes_cond[i, j].axis('off')\n",
    "                    else:\n",
    "                        axes_cond[j].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Campioni dal modello condizionato\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "# PARTE 10: ESEMPIO ESECUZIONE DEL MODELLO\n",
    "\n",
    "def run_latent_quantum_model(use_conditioning=False, dataset_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Esempio di utilizzo del modello quantistico latente, seguendo i parametri del paper.\n",
    "    \n",
    "    Args:\n",
    "        use_conditioning: Se True, implementa il modello condizionato\n",
    "        dataset_fraction: Frazione del dataset da utilizzare (0.1 = 10%)\n",
    "        \n",
    "    Returns:\n",
    "        params: Parametri addestrati\n",
    "        latent_samples: Campioni generati nello spazio latente\n",
    "        generated_samples: Campioni generati nello spazio originale\n",
    "        autoencoder: Autoencoder utilizzato per la codifica/decodifica\n",
    "    \"\"\"\n",
    "    # Verifica GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Esecuzione del modello latente su: {device}\")\n",
    "\n",
    "    # Preprocessa il dataset MNIST per il modello latente\n",
    "    X_train_latent, X_test_latent, y_train, y_test, autoencoder = preprocess_mnist_for_latent_model(\n",
    "        latent_dim=8, fraction=dataset_fraction\n",
    "    )\n",
    "\n",
    "    # Converti y_train e y_test da Pandas Series a NumPy array se necessario\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train = y_train.to_numpy()\n",
    "\n",
    "    if isinstance(y_test, pd.Series):\n",
    "        y_test = y_test.to_numpy()\n",
    "\n",
    "    # Parametri dal paper\n",
    "    n_qubits = 3  # 3 qubit per codificare lo spazio latente\n",
    "    m_ancilla = 1  # 1 qubit ancillare\n",
    "    n_layers = latent_n_layers  \n",
    "    n_label_qubits = 4 if use_conditioning else 0  # 4 qubit per etichette se condizionato\n",
    "\n",
    "    # Hyperparameters\n",
    "    timesteps = 8\n",
    "    n_epochs = 5\n",
    "    batch_size = 64\n",
    "\n",
    "    # Addestramento del modello\n",
    "    print(f\"Addestramento del modello quantum diffusion {'condizionato ' if use_conditioning else ''}latente...\")\n",
    "    try:\n",
    "        params_trained, losses = train_quantum_diffusion(\n",
    "            X_train_latent, y_train=y_train if use_conditioning else None,\n",
    "            n_qubits=n_qubits, m_ancilla=m_ancilla, n_layers=n_layers,\n",
    "            use_labels=use_conditioning, n_label_qubits=n_label_qubits,\n",
    "            n_epochs=n_epochs, batch_size=batch_size\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'addestramento: {str(e)}\")\n",
    "        total_params, _ = calculate_circuit_parameters(\n",
    "            n_qubits, m_ancilla, n_layers, use_conditioning, n_label_qubits)\n",
    "        params_trained = np.random.uniform(0, 2*np.pi, total_params)\n",
    "        losses = [1.0]\n",
    "\n",
    "    # Generazione di campioni\n",
    "    print(\"Generazione di campioni dal modello addestrato...\")\n",
    "    n_samples = 50\n",
    "\n",
    "    # Se condizionato, genera campioni per ciascuna classe\n",
    "    if use_conditioning:\n",
    "        labels = np.repeat(np.arange(10), n_samples // 10 + 1)[:n_samples]\n",
    "    else:\n",
    "        labels = None\n",
    "\n",
    "    # Genera i campioni nello spazio latente\n",
    "    try:\n",
    "        latent_samples = sample_from_model(\n",
    "            params_trained, n_samples,\n",
    "            n_qubits=n_qubits, m_ancilla=m_ancilla, n_layers=n_layers,\n",
    "            use_labels=use_conditioning, n_label_qubits=n_label_qubits,\n",
    "            labels=labels\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il sampling: {str(e)}\")\n",
    "        feature_dim = 2**n_qubits\n",
    "        latent_samples = np.random.rand(n_samples, feature_dim)\n",
    "        for i in range(n_samples):\n",
    "            latent_samples[i] = latent_samples[i] / np.sum(latent_samples[i])\n",
    "\n",
    "    # Decodifica i campioni latenti nello spazio originale\n",
    "    with torch.no_grad():\n",
    "        latent_samples_tensor = torch.tensor(latent_samples, dtype=torch.float32)\n",
    "        generated_samples = autoencoder.decode(latent_samples_tensor).numpy()\n",
    "\n",
    "    # Valutazione del modello\n",
    "    print(\"Valutazione del modello...\")\n",
    "\n",
    "    # Valutazione nello spazio latente\n",
    "    try:\n",
    "        fid_latent = calculate_frechet_inception_distance(X_test_latent, latent_samples)\n",
    "        wam_latent = calculate_wasserstein_distance_gaussian_mixture(X_test_latent, latent_samples)\n",
    "\n",
    "        print(f\"Metriche di valutazione nello spazio latente:\")\n",
    "        print(f\"Fréchet Inception Distance (FID): {fid_latent:.4f}\")\n",
    "        print(f\"Wasserstein distance for Gaussian mixture models (WaM): {wam_latent:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il calcolo delle metriche: {str(e)}\")\n",
    "\n",
    "    # Se condizionato, calcola anche il ROC-AUC\n",
    "    if use_conditioning and labels is not None:\n",
    "        try:\n",
    "            roc_auc_scores = calculate_roc_auc(latent_samples, labels)\n",
    "            print(f\"ROC-AUC scores per classe:\")\n",
    "            for i, score in enumerate(roc_auc_scores):\n",
    "                print(f\"Classe {i}: {score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il calcolo del ROC-AUC: {str(e)}\")\n",
    "\n",
    "    return params_trained, latent_samples, generated_samples, autoencoder\n",
    "\n",
    "# Esempio di utilizzo semplificato\n",
    "if __name__ == \"__main__\":\n",
    "    # Esegui un modello ridotto per test\n",
    "    print(\"Avvio del modello quantum diffusion su un sottoinsieme di MNIST...\")\n",
    "    \n",
    "    try:\n",
    "        # Esegui il modello non condizionato con una piccola frazione del dataset\n",
    "        params_uncond, latent_samples_uncond, samples_uncond, autoencoder_uncond = run_latent_quantum_model(\n",
    "            use_conditioning=False, dataset_fraction=0.1)\n",
    "        params_cond, latent_samples_cond, samples_cond, autoencoder_cond = run_latent_quantum_model(\n",
    "            use_conditioning=True, dataset_fraction=0.1)\n",
    "        \n",
    "        \n",
    "        # Visualizza i campioni generati\n",
    "        print(\"Visualizzazione dei campioni generati:\")\n",
    "        plot_generated_samples(samples_uncond=samples_uncond)\n",
    "        plot_generated_samples(samples_cond=samples_cond)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'esecuzione: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"Esecuzione completata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizzazione dei campioni generati:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x2000 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Visualizzazione dei campioni generati:\")\n",
    "plot_generated_samples(samples_cond=samples_cond)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
